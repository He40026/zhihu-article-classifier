import os
import shutil
import requests
import time
import json
import yaml
import re
import signal
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict

# è½»é‡çº§æ¨¡å¼é…ç½® - è®¾ç½®ä¸ºFalseå¯é¿å…pandasç­‰é‡å‹ä¾èµ–
ENABLE_ADVANCED_FEATURES = True  # æ˜¯å¦å¯ç”¨é«˜çº§åŠŸèƒ½ï¼ˆéœ€è¦pandasã€openpyxlç­‰ä¾èµ–ï¼‰

# æ¡ä»¶å¯¼å…¥pandasï¼ˆä»…åœ¨å¯ç”¨é«˜çº§åŠŸèƒ½æ—¶ï¼‰
if ENABLE_ADVANCED_FEATURES:
    try:
        import pandas as pd
        PANDAS_AVAILABLE = True
    except ImportError:
        print("è­¦å‘Š: pandasæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨è½»é‡çº§æ¨¡å¼")
        PANDAS_AVAILABLE = False
        ENABLE_ADVANCED_FEATURES = False
else:
    PANDAS_AVAILABLE = False


# AIé…ç½®å‚æ•°
OPENAI_API_KEY = "sk-sdadadawdada"  # OpenAI APIå¯†é’¥ï¼ˆå¿…éœ€é¡¹ï¼‰
OPENAI_BASE_URL = "https://api.vveai.com/v1"  # OpenAI Base API URLï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼ˆå¦‚ä½¿ç”¨ä»£ç†æˆ–å…¶ä»–å…¼å®¹çš„APIæœåŠ¡ï¼‰
OPENAI_MODEL = "gpt-4.1"

DEEPSEEK_API_KEY = "sk-test"  # DeepSeek APIå¯†é’¥ï¼ˆå¿…éœ€é¡¹ï¼‰
DEEPSEEK_BASE_URL = "https://api.deepseek.com"  # DeepSeek API Base URL
DEEPSEEK_MODEL = "deepseek-chat"  # DeepSeek æ¨¡å‹åç§°


# AIæœåŠ¡é…ç½®ç®¡ç†
CURRENT_AI_PROVIDER = "auto"  # å½“å‰ä½¿ç”¨çš„AIæœåŠ¡æä¾›å•†: "auto", "openai", "deepseek"
PREFERRED_AI_PROVIDER = "deepseek"  # é¦–é€‰çš„AIæœåŠ¡æä¾›å•†

# åº”ç”¨é…ç½®
SOURCE_DIR = "D:\Desktop\Zhihu-Collection-Downloader-main\docs\æˆ‘çš„æ”¶è—å¤¹"  # Markdownæ–‡ä»¶å­˜æ”¾è·¯å¾„ï¼Œé»˜è®¤ä¸ºæ ¹ç›®å½•
BASE_DIR = "D:\Desktop\Zhihu-Collection-Downloader-main\docs\åˆ†ç±»"  # åˆ†ç±»å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ä¸ºæ ¹ç›®å½•
CATEGORIES = ["çŸ¥è¯†å­¦ä¹ ", "ç§‘æŠ€äº’è”ç½‘", "äººæ–‡ç¤¾ç§‘", "å“²å­¦æ€è¾¨", "ä¸“ä¸šæŠ€æœ¯", "æƒ…æ„Ÿç”Ÿæ´»", "æ–‡å¨±è‰ºæœ¯", "è½»æ¾å¨±ä¹"]  # ä¼˜åŒ–åçš„åˆ†ç±»ä½“ç³»ï¼ˆ8ä¸ªæ¸…æ™°äº’æ–¥çš„ç±»åˆ«ï¼‰

# APIé…ç½®å‚æ•°
API_TIMEOUT = 30  # APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
ENABLE_DEBUG_OUTPUT = True  # æ˜¯å¦å¯ç”¨è°ƒè¯•è¾“å‡ºï¼ˆæ˜¾ç¤ºå‘é€ç»™AIçš„å†…å®¹ï¼‰

# ä¸­é€”åœæ­¢æ§åˆ¶
ENABLE_GRACEFUL_STOP = True  # æ˜¯å¦å¯ç”¨ä¼˜é›…åœæ­¢æœºåˆ¶
ENABLE_PAUSE_RESUME = True  # æ˜¯å¦å¯ç”¨æš‚åœ/ç»§ç»­åŠŸèƒ½
SAVE_PROGRESS_INTERVAL = 10  # æ¯å¤„ç†å¤šå°‘ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦

# æ–°å¢åŠŸèƒ½é…ç½®
ENABLE_CONTENT_ANALYSIS = True  # æ˜¯å¦å¯ç”¨å†…å®¹åˆ†æåŠŸèƒ½
ENABLE_TAG_EXTRACTION = True  # æ˜¯å¦å¯ç”¨æ ‡ç­¾æå–åŠŸèƒ½
ENABLE_RESULT_EXPORT = ENABLE_ADVANCED_FEATURES  # æ˜¯å¦å¯ç”¨ç»“æœå¯¼å‡ºåŠŸèƒ½ï¼ˆä¾èµ–pandasï¼‰
EXPORT_FORMAT = ["csv", "excel"] if ENABLE_ADVANCED_FEATURES else []  # å¯¼å‡ºæ ¼å¼ï¼Œå¯é€‰: "csv", "excel"
MAX_CONTENT_LENGTH = 2000  # åˆ†æçš„æ–‡ç« å†…å®¹æœ€å¤§å­—ç¬¦æ•°ï¼Œé¿å…APIè°ƒç”¨è¿‡é•¿

# åŠ¨æ€APIé…ç½®ï¼ˆå°†æ ¹æ®å½“å‰æä¾›å•†åŠ¨æ€è®¾ç½®ï¼‰
API_URL = ""
COMMON_HEADERS = {}

# é¢„å®šä¹‰æ ‡ç­¾ä½“ç³» - è§£å†³æ ‡ç­¾æ‚ä¹±é—®é¢˜
PREDEFINED_TAGS = {
    "çŸ¥è¯†å­¦ä¹ ": [
        "å­¦ä¹ æ–¹æ³•", "æ•™è‚²", "è¯»ä¹¦", "æ€ç»´è®­ç»ƒ", "è®°å¿†æŠ€å·§", "å­¦ä¹ ç»éªŒ", "çŸ¥è¯†ç®¡ç†",
        "è‡ªæˆ‘æå‡", "æŠ€èƒ½å­¦ä¹ ", "åœ¨çº¿æ•™è‚²", "åŸ¹è®­", "è€ƒè¯•", "å­¦æœ¯ç ”ç©¶", "ç»ˆèº«å­¦ä¹ "
    ],
    "ç§‘æŠ€äº’è”ç½‘": [
        "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "ç¼–ç¨‹", "è½¯ä»¶å¼€å‘", "äº’è”ç½‘", "ç§‘æŠ€è¶‹åŠ¿", "æ•°æ®ç§‘å­¦",
        "äº‘è®¡ç®—", "åŒºå—é“¾", "ç§»åŠ¨å¼€å‘", "å‰ç«¯å¼€å‘", "åç«¯å¼€å‘", "äº§å“ç»ç†", "ç§‘æŠ€è¯„æµ‹",
        "ç½‘ç»œå®‰å…¨", "å¤§æ•°æ®", "ç‰©è”ç½‘", "AR/VR", "é‡å­è®¡ç®—", "å¼€æºé¡¹ç›®"
    ],
    "äººæ–‡ç¤¾ç§‘": [
        "å†å²", "æ–‡å­¦", "ç¤¾ä¼šå­¦", "å¿ƒç†å­¦", "æ”¿æ²»", "ç»æµå­¦", "åœ°ç†", "äººç±»å­¦",
        "è¯­è¨€å­¦", "æ–‡åŒ–", "ç¤¾ä¼šç°è±¡", "ä¼ ç»Ÿæ–‡åŒ–", "å›½é™…å…³ç³»", "æ³•å¾‹", "æ–°é—»ä¼ æ’­"
    ],
    "å“²å­¦æ€è¾¨": [
        "å“²å­¦", "é€»è¾‘æ€ç»´", "äººç”Ÿæ„Ÿæ‚Ÿ", "ä»·å€¼è§‚", "ä¸–ç•Œè§‚", "è®¤çŸ¥ç§‘å­¦", "æ‰¹åˆ¤æ€ç»´",
        "é“å¾·ä¼¦ç†", "å­˜åœ¨ä¸»ä¹‰", "ç†æ€§æ€è€ƒ", "è¾©è¯æ³•", "æ€æƒ³å®éªŒ", "äººæ€§æ€è€ƒ"
    ],
    "ä¸“ä¸šæŠ€æœ¯": [
        "å·¥ç¨‹æŠ€æœ¯", "åŒ»å­¦", "é‡‘è", "æ³•å¾‹å®åŠ¡", "å»ºç­‘", "è®¾è®¡", "åˆ¶é€ ä¸š", "èƒ½æº",
        "ç¯ä¿", "å†œä¸š", "ç”Ÿç‰©æŠ€æœ¯", "åŒ–å­¦", "ç‰©ç†", "æ•°å­¦", "ç»Ÿè®¡å­¦", "é¡¹ç›®ç®¡ç†",
        "è´¨é‡ç®¡ç†", "ä¾›åº”é“¾", "åˆ›ä¸š", "å•†ä¸šæ¨¡å¼", "å¸‚åœºè¥é”€", "å“ç‰Œå»ºè®¾"
    ],
    "æƒ…æ„Ÿç”Ÿæ´»": [
        "æ‹çˆ±", "å©šå§»", "å®¶åº­", "äº²å­æ•™è‚²", "äººé™…å…³ç³»", "æƒ…æ„Ÿå’¨è¯¢", "å¿ƒç†å¥åº·",
        "ç¤¾äº¤æŠ€å·§", "ä¸ªäººæˆé•¿", "æƒ…ç»ªç®¡ç†", "å‹åŠ›ç¼“è§£", "ç”Ÿæ´»æ–¹å¼", "å¥åº·å…»ç”Ÿ",
        "ç¾é£Ÿ", "æ—…è¡Œ", "è¿åŠ¨å¥èº«", "æ—¶å°š", "ç¾å®¹æŠ¤è‚¤"
    ],
    "æ–‡å¨±è‰ºæœ¯": [
        "ç”µå½±", "éŸ³ä¹", "æ–‡å­¦ä½œå“", "è‰ºæœ¯", "ç»˜ç”»", "æ‘„å½±", "æˆå‰§", "èˆè¹ˆ",
        "ä¹¦è¯„", "å½±è¯„", "æ¸¸æˆ", "åŠ¨æ¼«", "æ–‡åŒ–åˆ›æ„", "è®¾è®¡ç¾å­¦", "æ”¶è—"
    ],
    "è½»æ¾å¨±ä¹": [
        "æç¬‘", "æ®µå­", "è¶£é—»", "å…«å¦", "ç½‘ç»œæ¢—", "ç”Ÿæ´»è¶£äº‹", "å® ç‰©", "ç¾é£Ÿåˆ†äº«",
        "æ—…è¡Œè§é—»", "æ—¥å¸¸ç”Ÿæ´»", "ä¼‘é—²å¨±ä¹", "å…´è¶£çˆ±å¥½", "æ”¾æ¾å¿ƒæƒ…", "æ²»æ„ˆç³»"
    ]
}

# æ ‡ç­¾è§„èŒƒåŒ–æ˜ å°„ - å°†ç›¸ä¼¼æ ‡ç­¾ç»Ÿä¸€
TAG_NORMALIZATION = {
    "AI": "äººå·¥æ™ºèƒ½", "ML": "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ": "æœºå™¨å­¦ä¹ ", "ç¥ç»ç½‘ç»œ": "æœºå™¨å­¦ä¹ ",
    "Python": "ç¼–ç¨‹", "Java": "ç¼–ç¨‹", "JavaScript": "ç¼–ç¨‹", "ä»£ç ": "ç¼–ç¨‹", "ç®—æ³•": "ç¼–ç¨‹",
    "å‰ç«¯": "å‰ç«¯å¼€å‘", "åç«¯": "åç«¯å¼€å‘", "å…¨æ ˆ": "è½¯ä»¶å¼€å‘", "APP": "ç§»åŠ¨å¼€å‘", "å°ç¨‹åº": "ç§»åŠ¨å¼€å‘",
    "åˆ›æ–°": "ç§‘æŠ€è¶‹åŠ¿", "æŠ€æœ¯": "ä¸“ä¸šæŠ€æœ¯", "å·¥ä½œ": "ä¸“ä¸šæŠ€æœ¯", "èŒåœº": "ä¸“ä¸šæŠ€æœ¯",
    "ç®¡ç†": "é¡¹ç›®ç®¡ç†", "é¢†å¯¼åŠ›": "é¡¹ç›®ç®¡ç†", "è¯»ä¹¦ç¬”è®°": "è¯»ä¹¦", "ä¹¦ç±": "è¯»ä¹¦",
    "å­¦ä¹ ç¬”è®°": "å­¦ä¹ æ–¹æ³•", "æ•ˆç‡": "å­¦ä¹ æ–¹æ³•", "æ—¶é—´ç®¡ç†": "å­¦ä¹ æ–¹æ³•",
    "æ€è€ƒ": "å“²å­¦æ€è¾¨", "æ„Ÿæ‚Ÿ": "äººç”Ÿæ„Ÿæ‚Ÿ", "ç”Ÿæ´»æ„Ÿæ‚Ÿ": "äººç”Ÿæ„Ÿæ‚Ÿ", "å¿ƒç†": "å¿ƒç†å­¦",
    "æ²Ÿé€š": "ç¤¾äº¤æŠ€å·§", "äº¤æµ": "ç¤¾äº¤æŠ€å·§", "å…³ç³»": "äººé™…å…³ç³»", "çˆ±æƒ…": "æ‹çˆ±",
    "å¥åº·": "å¥åº·å…»ç”Ÿ", "å…»ç”Ÿ": "å¥åº·å…»ç”Ÿ", "é”»ç‚¼": "è¿åŠ¨å¥èº«", "å¥èº«": "è¿åŠ¨å¥èº«",
    "ç”µå½±æ¨è": "ç”µå½±", "è§‚å½±": "ç”µå½±", "éŸ³ä¹æ¨è": "éŸ³ä¹", "æ­Œæ›²": "éŸ³ä¹",
    "æ¸¸æˆè¯„æµ‹": "æ¸¸æˆ", "ç©å®¶": "æ¸¸æˆ", "å¹½é»˜": "æç¬‘", "æœ‰è¶£": "è¶£é—»", "ç”Ÿæ´»": "æ—¥å¸¸ç”Ÿæ´»"
}

#---------------------------------------------------åˆ†å‰²çº¿-------------------------------------------------

class AIServiceManager:
    """AIæœåŠ¡ç®¡ç†å™¨ - ç®¡ç†OpenAIå’ŒDeepSeekæœåŠ¡çš„åˆ‡æ¢"""
    
    def __init__(self):
        self.current_provider = CURRENT_AI_PROVIDER
        self.preferred_provider = PREFERRED_AI_PROVIDER
        self.session = None
        self._initialize_service()
    
    def _initialize_service(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        try:
            provider = self._get_best_available_provider()
            self._setup_provider(provider)
            print(f"[AIæœåŠ¡] åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ {provider.upper()} æœåŠ¡")
        except Exception as e:
            print(f"[é”™è¯¯] AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _setup_provider(self, provider: str):
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„AIæœåŠ¡æä¾›å•†"""
        global API_URL, COMMON_HEADERS
        
        self.current_provider = provider
        
        if provider == "openai":
            API_URL = f"{OPENAI_BASE_URL}/chat/completions"
            COMMON_HEADERS = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
        elif provider == "deepseek":
            API_URL = f"{DEEPSEEK_BASE_URL}/v1/chat/completions"
            COMMON_HEADERS = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæœåŠ¡æä¾›å•†: {provider}")
        
        # åˆ›å»ºæ–°çš„ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(COMMON_HEADERS)
    
    def _test_provider_connection(self, provider: str) -> Tuple[bool, str]:
        """æµ‹è¯•æŒ‡å®šæä¾›å•†çš„è¿æ¥"""
        if provider == "openai":
            if not OPENAI_API_KEY.strip() or OPENAI_API_KEY == "sk-your-openai-api-key-here":
                return False, "OpenAI APIå¯†é’¥æœªé…ç½®"
            api_key = OPENAI_API_KEY
            base_url = OPENAI_BASE_URL
            model = OPENAI_MODEL
        elif provider == "deepseek":
            if not DEEPSEEK_API_KEY.strip() or DEEPSEEK_API_KEY == "sk-your-deepseek-api-key-here":
                return False, "DeepSeek APIå¯†é’¥æœªé…ç½®"
            api_key = DEEPSEEK_API_KEY
            base_url = DEEPSEEK_BASE_URL
            model = DEEPSEEK_MODEL
        else:
            return False, f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}"
        
        # æ„å»ºæµ‹è¯•è¯·æ±‚
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        test_payload = {
            "model": model,
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 5,
            "temperature": 0.1
        }
        
        try:
            url = f"{base_url}/v1/chat/completions" if provider == "deepseek" else f"{base_url}/chat/completions"
            response = requests.post(url, headers=headers, json=test_payload, timeout=10)
            
            if response.status_code == 200:
                return True, f"{provider} è¿æ¥æˆåŠŸ"
            elif response.status_code == 401:
                return False, f"{provider} APIå¯†é’¥æ— æ•ˆ"
            elif response.status_code == 429:
                return False, f"{provider} APIè°ƒç”¨é¢‘ç‡é™åˆ¶"
            else:
                return False, f"{provider} è¿æ¥å¤±è´¥: HTTP {response.status_code}"
        except requests.exceptions.Timeout:
            return False, f"{provider} è¿æ¥è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, f"{provider} ç½‘ç»œè¿æ¥é”™è¯¯"
        except Exception as e:
            return False, f"{provider} æµ‹è¯•å¤±è´¥: {str(e)}"
    
    def _get_best_available_provider(self) -> str:
        """è·å–æœ€ä½³å¯ç”¨çš„AIæœåŠ¡æä¾›å•†"""
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæä¾›å•†ä¸”ä¸æ˜¯autoï¼Œç›´æ¥æ£€æŸ¥è¯¥æä¾›å•†
        if self.current_provider != "auto":
            is_available, _ = self._test_provider_connection(self.current_provider)
            if is_available:
                return self.current_provider
            else:
                print(f"[è­¦å‘Š] æŒ‡å®šçš„æä¾›å•† {self.current_provider} ä¸å¯ç”¨ï¼Œå°è¯•è‡ªåŠ¨é€‰æ‹©")
        
        # è‡ªåŠ¨æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨preferred_provider
        is_available, _ = self._test_provider_connection(self.preferred_provider)
        if is_available:
            return self.preferred_provider
        
        # å¦‚æœpreferredä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æä¾›å•†
        all_providers = ["openai", "deepseek"]
        for provider in all_providers:
            if provider != self.preferred_provider:
                is_available, _ = self._test_provider_connection(provider)
                if is_available:
                    return provider
        
        raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡æä¾›å•†")
    
    def switch_provider(self, provider: str):
        """åˆ‡æ¢AIæœåŠ¡æä¾›å•†"""
        if provider not in ["auto", "openai", "deepseek"]:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæœåŠ¡æä¾›å•†: {provider}")
        
        old_provider = self.current_provider
        self.current_provider = provider
        
        try:
            if provider == "auto":
                actual_provider = self._get_best_available_provider()
                self._setup_provider(actual_provider)
                print(f"[åˆ‡æ¢] å·²åˆ‡æ¢åˆ°è‡ªåŠ¨æ¨¡å¼ï¼Œå½“å‰ä½¿ç”¨: {actual_provider.upper()}")
            else:
                is_available, message = self._test_provider_connection(provider)
                if not is_available:
                    raise RuntimeError(f"æ— æ³•åˆ‡æ¢åˆ° {provider}: {message}")
                self._setup_provider(provider)
                print(f"[åˆ‡æ¢] å·²åˆ‡æ¢åˆ°: {provider.upper()}")
        except Exception as e:
            # åˆ‡æ¢å¤±è´¥ï¼Œæ¢å¤åŸæ¥çš„æä¾›å•†
            self.current_provider = old_provider
            self._setup_provider(old_provider)
            raise e
    
    def set_preferred_provider(self, provider: str):
        """è®¾ç½®é¦–é€‰æä¾›å•†"""
        if provider not in ["openai", "deepseek"]:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæœåŠ¡æä¾›å•†: {provider}")
        
        self.preferred_provider = provider
        print(f"[é…ç½®] é¦–é€‰AIæœåŠ¡æä¾›å•†å·²è®¾ç½®ä¸º: {provider.upper()}")
    
    def get_current_provider_info(self) -> Tuple[str, str]:
        """è·å–å½“å‰ä½¿ç”¨çš„AIæœåŠ¡æä¾›å•†ä¿¡æ¯"""
        if self.current_provider == "openai":
            return "openai", OPENAI_MODEL
        elif self.current_provider == "deepseek":
            return "deepseek", DEEPSEEK_MODEL
        else:
            return "æœªçŸ¥", "æœªçŸ¥"
    
    def get_provider_status(self) -> Dict[str, Tuple[bool, str]]:
        """è·å–æ‰€æœ‰æä¾›å•†çš„çŠ¶æ€"""
        providers = ["openai", "deepseek"]
        status = {}
        
        for provider in providers:
            status[provider] = self._test_provider_connection(provider)
        
        return status
    
    def show_status(self):
        """æ˜¾ç¤ºAIæœåŠ¡çŠ¶æ€"""
        print("\n=== AIæœåŠ¡çŠ¶æ€ ===")
        print(f"å½“å‰è®¾ç½®: {self.current_provider}")
        print(f"é¦–é€‰æä¾›å•†: {self.preferred_provider}")
        
        # è·å–å½“å‰å®é™…ä½¿ç”¨çš„æä¾›å•†
        try:
            actual_provider = self._get_best_available_provider()
            print(f"å®é™…ä½¿ç”¨: {actual_provider.upper()}")
        except:
            print("å®é™…ä½¿ç”¨: æ— å¯ç”¨æœåŠ¡")
        
        print()
        
        # æ£€æŸ¥æ‰€æœ‰æä¾›å•†çŠ¶æ€
        provider_status = self.get_provider_status()
        
        for provider, (is_available, message) in provider_status.items():
            status_icon = "âœ…" if is_available else "âŒ"
            print(f"{status_icon} {provider.upper()}: {message}")
            
            if provider == "openai":
                api_key_preview = OPENAI_API_KEY[:10] + "..." if len(OPENAI_API_KEY) > 10 else OPENAI_API_KEY
                print(f"   æ¨¡å‹: {OPENAI_MODEL}")
                print(f"   APIå¯†é’¥: {api_key_preview}")
            elif provider == "deepseek":
                api_key_preview = DEEPSEEK_API_KEY[:10] + "..." if len(DEEPSEEK_API_KEY) > 10 else DEEPSEEK_API_KEY
                print(f"   æ¨¡å‹: {DEEPSEEK_MODEL}")
                print(f"   APIå¯†é’¥: {api_key_preview}")
            print()
    
    def interactive_setup(self):
        """äº¤äº’å¼è®¾ç½®å‘å¯¼"""
        print("\n=== AIæœåŠ¡é…ç½®å‘å¯¼ ===")
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        self.show_status()
        
        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. åˆ‡æ¢AIæœåŠ¡æä¾›å•†")
            print("2. è®¾ç½®é¦–é€‰æä¾›å•†")
            print("3. æµ‹è¯•è¿æ¥")
            print("4. æ˜¾ç¤ºçŠ¶æ€")
            print("0. è¿”å›")
            
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-4): ").strip()
            
            if choice == "1":
                self._switch_provider_menu()
            elif choice == "2":
                self._set_preferred_menu()
            elif choice == "3":
                self._test_connections()
            elif choice == "4":
                self.show_status()
            elif choice == "0":
                break
            else:
                print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
    
    def _switch_provider_menu(self):
        """åˆ‡æ¢æä¾›å•†èœå•"""
        print("\n=== åˆ‡æ¢AIæœåŠ¡æä¾›å•† ===")
        print("1. è‡ªåŠ¨é€‰æ‹© (æ¨è)")
        print("2. å›ºå®šä½¿ç”¨ OpenAI")
        print("3. å›ºå®šä½¿ç”¨ DeepSeek")
        
        choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
        
        try:
            if choice == "1":
                self.switch_provider("auto")
                print("âœ… å·²è®¾ç½®ä¸ºè‡ªåŠ¨é€‰æ‹©æ¨¡å¼")
            elif choice == "2":
                self.switch_provider("openai")
                print("âœ… å·²è®¾ç½®ä¸ºå›ºå®šä½¿ç”¨ OpenAI")
            elif choice == "3":
                self.switch_provider("deepseek")
                print("âœ… å·²è®¾ç½®ä¸ºå›ºå®šä½¿ç”¨ DeepSeek")
            else:
                print("æ— æ•ˆé€‰é¡¹")
        except Exception as e:
            print(f"âŒ åˆ‡æ¢å¤±è´¥: {e}")
    
    def _set_preferred_menu(self):
        """è®¾ç½®é¦–é€‰æä¾›å•†èœå•"""
        print("\n=== è®¾ç½®é¦–é€‰æä¾›å•† ===")
        print("(è‡ªåŠ¨æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨)")
        print("1. OpenAI")
        print("2. DeepSeek")
        
        choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
        
        if choice == "1":
            self.set_preferred_provider("openai")
            print("âœ… é¦–é€‰æä¾›å•†å·²è®¾ç½®ä¸º OpenAI")
        elif choice == "2":
            self.set_preferred_provider("deepseek")
            print("âœ… é¦–é€‰æä¾›å•†å·²è®¾ç½®ä¸º DeepSeek")
        else:
            print("æ— æ•ˆé€‰é¡¹")
    
    def _test_connections(self):
        """æµ‹è¯•æ‰€æœ‰è¿æ¥"""
        print("\n=== æµ‹è¯•APIè¿æ¥ ===")
        
        provider_status = self.get_provider_status()
        
        for provider, (is_available, message) in provider_status.items():
            status_icon = "âœ…" if is_available else "âŒ"
            print(f"{status_icon} {provider.upper()}: {message}")
        
        # æ˜¾ç¤ºå½“å‰æœ€ä½³æä¾›å•†
        try:
            best_provider = self._get_best_available_provider()
            print(f"\nğŸ”¥ å½“å‰æœ€ä½³å¯ç”¨æä¾›å•†: {best_provider.upper()}")
        except Exception as e:
            print(f"\nâš ï¸  {e}")

# åˆ›å»ºå…¨å±€AIæœåŠ¡ç®¡ç†å™¨
ai_service_manager = AIServiceManager()

# åˆ›å»ºå…¨å±€çš„ requests.Session å¯¹è±¡
def create_api_session():
    """åˆ›å»ºé…ç½®å¥½çš„APIä¼šè¯å¯¹è±¡"""
    return ai_service_manager.session

API_SESSION = create_api_session()

# å…¨å±€åœæ­¢æ§åˆ¶å˜é‡
stop_processing = threading.Event()
pause_processing = threading.Event()
progress_save_lock = threading.Lock()

class GracefulStopHandler:
    """ä¼˜é›…åœæ­¢å¤„ç†å™¨"""
    
    def __init__(self):
        self.stop_requested = False
        self.pause_requested = False
        
    def signal_handler(self, signum, frame):
        """å¤„ç†Ctrl+Cä¿¡å·"""
        print(f"\n[è­¦å‘Š] æ¥æ”¶åˆ°åœæ­¢ä¿¡å· (ä¿¡å·: {signum})")
        print("æ­£åœ¨ä¼˜é›…åœæ­¢å¤„ç†...")
        print("è¯·ç¨ç­‰ï¼Œæ­£åœ¨å®Œæˆå½“å‰æ–‡ä»¶çš„å¤„ç†...")
        stop_processing.set()
        self.stop_requested = True
        
    def setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        if ENABLE_GRACEFUL_STOP:
            signal.signal(signal.SIGINT, self.signal_handler)
            if hasattr(signal, 'SIGTERM'):
                signal.signal(signal.SIGTERM, self.signal_handler)

# å…¨å±€åœæ­¢å¤„ç†å™¨å®ä¾‹
stop_handler = GracefulStopHandler()



@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    filename: str
    original_title: str
    category: str
    tags: List[str]
    process_time: float
    success: bool
    error_message: Optional[str] = None
    content_preview: Optional[str] = None
    # Frontmatterå…ƒæ•°æ®å­—æ®µ
    title: Optional[str] = None
    url: Optional[str] = None
    author: Optional[str] = None
    author_badge: Optional[str] = None
    created: Optional[str] = None
    modified: Optional[str] = None
    upvote_num: Optional[int] = None
    comment_num: Optional[int] = None
    # ç»Ÿè®¡åˆ†æå­—æ®µ
    word_count: Optional[int] = None
    content_summary: Optional[str] = None
    processing_status: Optional[str] = None


class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨ - å¤„ç†è¿›åº¦ä¿å­˜å’Œæ¢å¤"""
    
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.progress_file = os.path.join(base_dir, ".classification_progress.json")
        self.processed_files = set()
        self.failed_files = set()
        
    def load_progress(self) -> Tuple[set, set]:
        """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_files = set(data.get('processed_files', []))
                    self.failed_files = set(data.get('failed_files', []))
                    print(f"[æ–‡ä»¶] åŠ è½½è¿›åº¦: å·²å¤„ç† {len(self.processed_files)} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥ {len(self.failed_files)} ä¸ªæ–‡ä»¶")
                    return self.processed_files, self.failed_files
            except Exception as e:
                print(f"[è­¦å‘Š] åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        return set(), set()
    
    def save_progress(self, processed_files: set, failed_files: set):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        try:
            with progress_save_lock:
                progress_data = {
                    'processed_files': list(processed_files),
                    'failed_files': list(failed_files),
                    'last_save_time': datetime.now().isoformat(),
                    'total_processed': len(processed_files) + len(failed_files)
                }
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[è­¦å‘Š] ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def clear_progress(self):
        """æ¸…é™¤è¿›åº¦æ–‡ä»¶"""
        if os.path.exists(self.progress_file):
            try:
                os.remove(self.progress_file)
                print("[åˆ é™¤] å·²æ¸…é™¤è¿›åº¦æ–‡ä»¶")
            except Exception as e:
                print(f"[è­¦å‘Š] æ¸…é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")


class UserInputHandler:
    """ç”¨æˆ·è¾“å…¥å¤„ç†å™¨ - å¤„ç†è¿è¡Œæ—¶ç”¨æˆ·äº¤äº’"""
    
    def __init__(self):
        self.input_thread = None
        self.running = False
        
    def start_input_monitoring(self):
        """å¼€å§‹ç›‘å¬ç”¨æˆ·è¾“å…¥"""
        if not ENABLE_PAUSE_RESUME:
            return
            
        self.running = True
        self.input_thread = threading.Thread(target=self._input_loop, daemon=True)
        self.input_thread.start()
        print("[æç¤º] å¤„ç†è¿‡ç¨‹ä¸­å¯ä»¥æŒ‰ä»¥ä¸‹é”®æ“ä½œ:")
        print("   - 'p' + Enter: æš‚åœå¤„ç†")
        print("   - 'r' + Enter: ç»§ç»­å¤„ç†")
        print("   - 'q' + Enter: ä¼˜é›…åœæ­¢")
        print("   - Ctrl+C: å¼ºåˆ¶åœæ­¢")
        print()
    
    def stop_input_monitoring(self):
        """åœæ­¢ç›‘å¬ç”¨æˆ·è¾“å…¥"""
        self.running = False
        
    def _input_loop(self):
        """è¾“å…¥ç›‘å¬å¾ªç¯"""
        while self.running:
            try:
                user_input = input().strip().lower()
                if user_input == 'p':
                    print("[æš‚åœ] æš‚åœå¤„ç†ä¸­...")
                    pause_processing.set()
                elif user_input == 'r':
                    print("[ç»§ç»­] ç»§ç»­å¤„ç†...")
                    pause_processing.clear()
                elif user_input == 'q':
                    print("[åœæ­¢] ç”¨æˆ·è¯·æ±‚åœæ­¢...")
                    stop_processing.set()
                    break
                elif user_input == 'help' or user_input == 'h':
                    print("å¯ç”¨å‘½ä»¤: p(æš‚åœ), r(ç»§ç»­), q(åœæ­¢), help(å¸®åŠ©)")
            except EOFError:
                break
            except Exception:
                continue


class ResultManager:
    """ç»“æœç®¡ç†å™¨"""
    
    def __init__(self):
        self.results: List[ProcessResult] = []
        self.start_time = datetime.now()
    
    def add_result(self, result: ProcessResult):
        """æ·»åŠ å¤„ç†ç»“æœ"""
        self.results.append(result)
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_files = len(self.results)
        success_count = sum(1 for r in self.results if r.success)
        failure_count = total_files - success_count
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for result in self.results:
            if result.success:
                category = result.category
                if category not in category_stats:
                    category_stats[category] = 0
                category_stats[category] += 1
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        if self.results:
            process_times = [r.process_time for r in self.results]
            avg_time = sum(process_times) / len(process_times)
            max_time = max(process_times)
            min_time = min(process_times)
        else:
            avg_time = max_time = min_time = 0
        
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        return {
            "total_files": total_files,
            "success_count": success_count,
            "failure_count": failure_count,
            "success_rate": success_count / total_files if total_files > 0 else 0,
            "category_distribution": category_stats,
            "processing_time": {
                "total_time": total_time,
                "average_per_file": avg_time,
                "max_time": max_time,
                "min_time": min_time
            }
        }
    
    def export_results(self, output_dir: str = "./"):
        """å¯¼å‡ºç»“æœ"""
        if not ENABLE_RESULT_EXPORT or not self.results:
            return
        
        if not PANDAS_AVAILABLE:
            # è½»é‡çº§æ¨¡å¼ï¼šå¯¼å‡ºç®€å•çš„æ–‡æœ¬æŠ¥å‘Š
            self._export_text_report(output_dir)
            return
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = []
        for result in self.results:
            export_data.append({
                # åŸºç¡€åˆ—
                "æ–‡ä»¶å": result.filename,
                "æ ‡é¢˜": result.title or result.original_title,
                "URL": result.url or "",
                "åˆ†ç±»ç»“æœ": result.category,
                "æå–çš„æ ‡ç­¾": ", ".join(result.tags) if result.tags else "",
                
                # å…ƒæ•°æ®åˆ—
                "ä½œè€…": result.author or "",
                "ä½œè€…è®¤è¯": result.author_badge or "",
                "åˆ›å»ºæ—¶é—´": result.created or "",
                "ä¿®æ”¹æ—¶é—´": result.modified or "",
                "ç‚¹èµæ•°": result.upvote_num if result.upvote_num is not None else "",
                "è¯„è®ºæ•°": result.comment_num if result.comment_num is not None else "",
                
                # ç»Ÿè®¡åˆ†æåˆ—
                "å­—æ•°ç»Ÿè®¡": result.word_count if result.word_count is not None else "",
                "å†…å®¹æ‘˜è¦": result.content_summary or "",
                "å¤„ç†æ—¶é—´": round(result.process_time, 3),
                "å¤„ç†çŠ¶æ€": "æˆåŠŸ" if result.success else "å¤±è´¥",
                "é”™è¯¯ä¿¡æ¯": result.error_message or ""
            })
        
        df = pd.DataFrame(export_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¯¼å‡ºCSV
        if "csv" in EXPORT_FORMAT:
            csv_path = os.path.join(output_dir, f"åˆ†ç±»ç»“æœ_{timestamp}.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"CSVç»“æœå·²å¯¼å‡ºåˆ°: {csv_path}")
        
        # å¯¼å‡ºExcel
        if "excel" in EXPORT_FORMAT:
            excel_path = os.path.join(output_dir, f"åˆ†ç±»ç»“æœ_{timestamp}.xlsx")
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='åˆ†ç±»ç»“æœ', index=False)
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯å·¥ä½œè¡¨
                stats = self.get_statistics()
                stats_data = []
                stats_data.append(["ç»Ÿè®¡é¡¹ç›®", "æ•°å€¼"])
                stats_data.append(["æ€»æ–‡ä»¶æ•°", stats["total_files"]])
                stats_data.append(["æˆåŠŸå¤„ç†", stats["success_count"]])
                stats_data.append(["å¤±è´¥å¤„ç†", stats["failure_count"]])
                stats_data.append(["æˆåŠŸç‡", f"{stats['success_rate']:.2%}"])
                stats_data.append(["æ€»å¤„ç†æ—¶é—´(ç§’)", f"{stats['processing_time']['total_time']:.2f}"])
                stats_data.append(["å¹³å‡å¤„ç†æ—¶é—´(ç§’)", f"{stats['processing_time']['average_per_file']:.3f}"])
                
                stats_df = pd.DataFrame(stats_data[1:], columns=stats_data[0])
                stats_df.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
                
                # æ·»åŠ åˆ†ç±»åˆ†å¸ƒå·¥ä½œè¡¨
                if stats["category_distribution"]:
                    category_data = [(k, v) for k, v in stats["category_distribution"].items()]
                    category_df = pd.DataFrame(category_data, columns=["åˆ†ç±»", "æ–‡ç« æ•°é‡"])
                    category_df.to_excel(writer, sheet_name='åˆ†ç±»åˆ†å¸ƒ', index=False)
            
            print(f"Excelç»“æœå·²å¯¼å‡ºåˆ°: {excel_path}")
    
    def _export_text_report(self, output_dir: str = "./"):
        """è½»é‡çº§æ¨¡å¼ï¼šå¯¼å‡ºæ–‡æœ¬æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(output_dir, f"åˆ†ç±»ç»“æœ_{timestamp}.txt")
        
        stats = self.get_statistics()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== çŸ¥ä¹æ–‡ç« åˆ†ç±»ç»“æœæŠ¥å‘Š ===\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            f.write("=== ç»Ÿè®¡æ‘˜è¦ ===\n")
            f.write(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}\n")
            f.write(f"æˆåŠŸå¤„ç†: {stats['success_count']}\n")
            f.write(f"å¤±è´¥å¤„ç†: {stats['failure_count']}\n")
            f.write(f"æˆåŠŸç‡: {stats['success_rate']:.2%}\n")
            f.write(f"æ€»å¤„ç†æ—¶é—´: {stats['processing_time']['total_time']:.2f} ç§’\n")
            f.write(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['processing_time']['average_per_file']:.3f} ç§’/æ–‡ä»¶\n\n")
            
            # åˆ†ç±»åˆ†å¸ƒ
            if stats['category_distribution']:
                f.write("=== åˆ†ç±»åˆ†å¸ƒ ===\n")
                for category, count in sorted(stats['category_distribution'].items(), key=lambda x: x[1], reverse=True):
                    f.write(f"{category}: {count} ç¯‡\n")
                f.write("\n")
            
            # è¯¦ç»†ç»“æœ
            f.write("=== è¯¦ç»†å¤„ç†ç»“æœ ===\n")
            for result in self.results:
                f.write(f"æ–‡ä»¶: {result.filename}\n")
                f.write(f"æ ‡é¢˜: {result.title or result.original_title}\n")
                f.write(f"åˆ†ç±»: {result.category}\n")
                if result.tags:
                    f.write(f"æ ‡ç­¾: {', '.join(result.tags)}\n")
                f.write(f"çŠ¶æ€: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}\n")
                if result.error_message:
                    f.write(f"é”™è¯¯: {result.error_message}\n")
                f.write(f"å¤„ç†æ—¶é—´: {result.process_time:.3f} ç§’\n")
                f.write("-" * 50 + "\n")
        
        print(f"æ–‡æœ¬æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {report_path}")


def parse_frontmatter(content: str) -> Tuple[Dict[str, Any], str]:
    """è§£æYAML frontmatterï¼Œè¿”å›å…ƒæ•°æ®å­—å…¸å’Œæ­£æ–‡å†…å®¹"""
    frontmatter_data = {}
    body_content = content
    
    # æ£€æŸ¥æ˜¯å¦æœ‰frontmatterï¼ˆä»¥---å¼€å§‹å’Œç»“æŸï¼‰
    if content.startswith('---\n'):
        try:
            # æŸ¥æ‰¾ç¬¬äºŒä¸ª---çš„ä½ç½®
            end_marker = content.find('\n---\n', 4)
            if end_marker != -1:
                # æå–frontmatteréƒ¨åˆ†
                frontmatter_str = content[4:end_marker]
                # è§£æYAML
                frontmatter_data = yaml.safe_load(frontmatter_str) or {}
                # æå–æ­£æ–‡å†…å®¹
                body_content = content[end_marker + 5:].strip()
            else:
                # æ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼ŒæŸ¥æ‰¾å•ç‹¬çš„---è¡Œ
                end_marker = content.find('\n---', 4)
                if end_marker != -1:
                    frontmatter_str = content[4:end_marker]
                    frontmatter_data = yaml.safe_load(frontmatter_str) or {}
                    body_content = content[end_marker + 4:].strip()
        except yaml.YAMLError as e:
            print(f"YAMLè§£æé”™è¯¯: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸å’ŒåŸå§‹å†…å®¹
            frontmatter_data = {}
            body_content = content
    
    return frontmatter_data, body_content


def normalize_tags(tags: List[str], category: str) -> List[str]:
    """æ ‡ç­¾è§„èŒƒåŒ–ï¼šå°†AIç”Ÿæˆçš„æ ‡ç­¾è½¬æ¢ä¸ºé¢„å®šä¹‰æ ‡ç­¾"""
    if not tags:
        return []
    
    normalized_tags = []
    available_tags = PREDEFINED_TAGS.get(category, [])
    
    for tag in tags:
        tag = tag.strip()
        if not tag:
            continue
            
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ¹é…é¢„å®šä¹‰æ ‡ç­¾
        if tag in available_tags:
            normalized_tags.append(tag)
            continue
            
        # 2. æ£€æŸ¥æ ‡ç­¾è§„èŒƒåŒ–æ˜ å°„
        if tag in TAG_NORMALIZATION:
            normalized_tag = TAG_NORMALIZATION[tag]
            if normalized_tag in available_tags:
                normalized_tags.append(normalized_tag)
                continue
        
        # 3. æ¨¡ç³ŠåŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«é¢„å®šä¹‰æ ‡ç­¾çš„å…³é”®è¯
        matched = False
        for predefined_tag in available_tags:
            if (tag in predefined_tag or predefined_tag in tag or
                any(keyword in tag for keyword in predefined_tag.split()) or
                any(keyword in predefined_tag for keyword in tag.split())):
                normalized_tags.append(predefined_tag)
                matched = True
                break
        
        # 4. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œæ£€æŸ¥å…¶ä»–åˆ†ç±»çš„æ ‡ç­¾ï¼ˆå¯èƒ½åˆ†ç±»ä¸å‡†ç¡®ï¼‰
        if not matched:
            for cat, cat_tags in PREDEFINED_TAGS.items():
                for predefined_tag in cat_tags:
                    if (tag in predefined_tag or predefined_tag in tag or
                        any(keyword in tag for keyword in predefined_tag.split()) or
                        any(keyword in predefined_tag for keyword in tag.split())):
                        normalized_tags.append(predefined_tag)
                        matched = True
                        break
                if matched:
                    break
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    normalized_tags = list(dict.fromkeys(normalized_tags))  # ä¿æŒé¡ºåºå»é‡
    return normalized_tags[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾


def clean_content(content: str) -> str:
    """æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹"""
    if not content:
        return ""
    
    # åŸºæœ¬å†…å®¹æ¸…ç†ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„è¡Œ
    lines = content.split('\n')
    clean_lines = []
    
    for line in lines:
        # ä¿ç•™åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡å­—ç¬¦çš„æœ‰æ„ä¹‰è¡Œ
        if re.search(r'[\u4e00-\u9fff]|[a-zA-Z]{3,}', line):  # åŒ…å«ä¸­æ–‡æˆ–3ä¸ªä»¥ä¸Šè‹±æ–‡å­—ç¬¦
            clean_lines.append(line)
    
    return '\n'.join(clean_lines)


def read_markdown_with_frontmatter(file_path: str) -> Tuple[str, Dict[str, Any]]:
    """è¯»å–Markdownæ–‡ä»¶å†…å®¹å¹¶è§£æfrontmatter"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin1']
        raw_content = ""
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    raw_content = f.read()
                break
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        if not raw_content:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œå°è¯•çš„æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥")
            return "", {}
        
        # è§£æfrontmatter
        frontmatter_data, body_content = parse_frontmatter(raw_content)
        
        # æ¸…ç†å†…å®¹
        body_content = clean_content(body_content)
        
        # ç§»é™¤markdownæ ¼å¼æ ‡è®°ï¼Œåªä¿ç•™æ–‡æœ¬å†…å®¹
        content = re.sub(r'#+\s*', '', body_content)  # æ ‡é¢˜
        content = re.sub(r'\*\*(.*?)\*\*', r'\1', content)  # ç²—ä½“
        content = re.sub(r'\*(.*?)\*', r'\1', content)  # æ–œä½“
        content = re.sub(r'`(.*?)`', r'\1', content)  # ä»£ç 
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)  # é“¾æ¥
        content = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', content)  # å›¾ç‰‡
        content = re.sub(r'\n+', '\n', content)  # å¤šä¸ªæ¢è¡Œåˆå¹¶
        
        # æœ€ç»ˆæ¸…ç†
        content = content.strip()
        
        # é™åˆ¶å†…å®¹é•¿åº¦
        if len(content) > MAX_CONTENT_LENGTH:
            content = content[:MAX_CONTENT_LENGTH] + "..."
        
        if ENABLE_DEBUG_OUTPUT and content:
            tqdm.write(f"DEBUG - æ¸…ç†åå†…å®¹é•¿åº¦: {len(content)}")
            tqdm.write(f"DEBUG - æ¸…ç†åå†…å®¹é¢„è§ˆ: {content[:100]}...")
        
        return content, frontmatter_data
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {file_path} - {str(e)}")
        return "", {}


def read_markdown_content(file_path: str) -> str:
    """è¯»å–Markdownæ–‡ä»¶å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    content, _ = read_markdown_with_frontmatter(file_path)
    return content


@retry(
    stop=stop_after_attempt(3),  # å‡å°‘é‡è¯•æ¬¡æ•°åˆ°3æ¬¡ï¼Œé¿å…è¿‡åº¦é‡è¯•
    wait=wait_exponential(multiplier=2, min=3, max=15),  # æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼š3s, 6s, 12s
    retry=retry_if_exception_type((
        requests.exceptions.Timeout,
        requests.exceptions.ConnectionError,
        requests.exceptions.HTTPError,
        requests.exceptions.ProxyError,
        requests.exceptions.ChunkedEncodingError,
        requests.exceptions.RequestException
    ))  # æ‰©å±•é‡è¯•çš„å¼‚å¸¸ç±»å‹
)
def classify_with_content_and_tags(title: str, content: str = "") -> Tuple[str, List[str]]:
    """ä½¿ç”¨AI APIè¿›è¡Œåˆ†ç±»å’Œæ ‡ç­¾æå–ï¼ˆå¸¦é‡è¯•ï¼‰"""
    
    # è·å–å½“å‰ä½¿ç”¨çš„AIæœåŠ¡ä¿¡æ¯
    current_provider, current_model = ai_service_manager.get_current_provider_info()
    
    # æ„å»ºç³»ç»Ÿæç¤º
    analysis_text = f"æ ‡é¢˜: {title}"
    if ENABLE_CONTENT_ANALYSIS and content:
        analysis_text += f"\n\nå†…å®¹: {content}"
    
    if ENABLE_TAG_EXTRACTION:
        SYSTEM_PROMPT = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡ç« åˆ†ç±»å™¨ã€‚è¯·å°†æ–‡ç« åˆ†ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š{', '.join(CATEGORIES)}

å¯¹äºæ ‡ç­¾æå–ï¼Œè¯·å°½é‡ä½¿ç”¨å‡†ç¡®æè¿°æ–‡ç« å†…å®¹çš„å…³é”®è¯ï¼Œé¿å…è¿‡äºå®½æ³›æˆ–æ¨¡ç³Šçš„æ ‡ç­¾ã€‚

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€åˆ†ææˆ–å…¶ä»–æ–‡å­—ï¼š
{{
    "category": "åˆ†ç±»åç§°",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]
}}

è¦æ±‚ï¼š
- åªè¾“å‡ºä¸Šè¿°JSONæ ¼å¼
- ä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—
- categoryå¿…é¡»æ˜¯æä¾›çš„ç±»åˆ«ä¹‹ä¸€
- tagsæå–3-5ä¸ªå…·ä½“ã€å‡†ç¡®çš„å…³é”®è¯
- æ ‡ç­¾åº”è¯¥å…·ä½“æè¿°æ–‡ç« ä¸»é¢˜ï¼Œé¿å…å¤ªå®½æ³›çš„è¯æ±‡"""
    else:
        SYSTEM_PROMPT = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡ç« åˆ†ç±»å™¨ã€‚å°†æ–‡ç« åˆ†ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š{', '.join(CATEGORIES)}

åªè¿”å›åˆ†ç±»åç§°ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

    payload = {
        "model": current_model,
        "messages": [{"role": "system", "content": SYSTEM_PROMPT},
                     {"role": "user", "content": analysis_text}],
        "temperature": 0.1,
        "max_tokens": 150  # å‡å°‘max_tokensï¼Œå¼ºåˆ¶AIç®€æ´å›åº”
    }

    try:
        # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºå‘é€ç»™AIçš„å†…å®¹
        if ENABLE_DEBUG_OUTPUT:
            tqdm.write(f"[AIè¯·æ±‚] ä½¿ç”¨ {current_provider.upper()} - {current_model}")
            tqdm.write(f"DEBUG - å‘é€ç»™AIçš„å†…å®¹é•¿åº¦: {len(analysis_text)}")
            tqdm.write(f"DEBUG - å†…å®¹é¢„è§ˆ: {analysis_text[:200]}...")
        
        response = ai_service_manager.session.post(API_URL, json=payload, timeout=API_TIMEOUT)
        response.encoding = 'utf-8'
        
        # è¯¦ç»†çš„å“åº”çŠ¶æ€æ£€æŸ¥
        tqdm.write(f"[æ£€æŸ¥] APIå“åº”çŠ¶æ€: {response.status_code}")
        if response.status_code != 200:
            tqdm.write(f"[é”™è¯¯] é200å“åº”: {response.text[:500]}")
            response.raise_for_status()

        # å°è¯•è§£æJSONå“åº”
        try:
            data = response.json()
        except json.JSONDecodeError as e:
            tqdm.write(f"[é”™è¯¯] å“åº”JSONè§£æå¤±è´¥: {e}")
            tqdm.write(f"[å†…å®¹] åŸå§‹å“åº”å†…å®¹: {response.text[:500]}")
            raise
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if not data:
            tqdm.write(f"[é”™è¯¯] ç©ºçš„å“åº”æ•°æ®")
            return "å…¶ä»–", []
            
        if 'choices' not in data:
            tqdm.write(f"[é”™è¯¯] å“åº”ä¸­æ²¡æœ‰'choices'å­—æ®µ: {data}")
            return "å…¶ä»–", []
            
        if not data['choices']:
            tqdm.write(f"[é”™è¯¯] choicesæ•°ç»„ä¸ºç©º: {data}")
            return "å…¶ä»–", []
            
        if 'message' not in data['choices'][0]:
            tqdm.write(f"[é”™è¯¯] ç¬¬ä¸€ä¸ªchoiceä¸­æ²¡æœ‰'message'å­—æ®µ: {data['choices'][0]}")
            return "å…¶ä»–", []
            
        if 'content' not in data['choices'][0]['message']:
            tqdm.write(f"[é”™è¯¯] messageä¸­æ²¡æœ‰'content'å­—æ®µ: {data['choices'][0]['message']}")
            return "å…¶ä»–", []

        result_text = data['choices'][0]['message']['content'].strip()
        
        # æ€»æ˜¯è¾“å‡ºAIçš„åŸå§‹å›å¤
        tqdm.write(f"[AI] AIåŸå§‹å›å¤: {result_text}")
        
        if ENABLE_TAG_EXTRACTION:
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
                result_json = json.loads(result_text)
                category = result_json.get("category", "å…¶ä»–")
                tags = result_json.get("tags", [])
                
                tqdm.write(f"[æˆåŠŸ] JSONè§£ææˆåŠŸ: category={category}, tags={tags}")
                
                # éªŒè¯åˆ†ç±»æ˜¯å¦æœ‰æ•ˆ
                if category not in CATEGORIES:
                    tqdm.write(f"[è­¦å‘Š] æ— æ•ˆåˆ†ç±» '{category}'ï¼Œä½¿ç”¨'å…¶ä»–'ä»£æ›¿")
                    category = "å…¶ä»–"
                
                return category, tags
                
            except json.JSONDecodeError as e:
                tqdm.write(f"[è­¦å‘Š] ç›´æ¥JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•æå–JSONç‰‡æ®µ")
                
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                try:
                    # å°è¯•æå–```json```ä»£ç å—ä¸­çš„å†…å®¹
                    json_match = re.search(r'```json\s*\n(.*?)\n```', result_text, re.DOTALL)
                    if json_match:
                        json_text = json_match.group(1).strip()
                        tqdm.write(f"[æå–] æå–åˆ°jsonä»£ç å—: {json_text}")
                        result_json = json.loads(json_text)
                        category = result_json.get("category", "å…¶ä»–")
                        tags = result_json.get("tags", [])
                        if category not in CATEGORIES:
                            category = "å…¶ä»–"
                        tqdm.write(f"[æˆåŠŸ] ä»£ç å—è§£ææˆåŠŸ: category={category}, tags={tags}")
                        return category, tags
                    
                    # å°è¯•æå–èŠ±æ‹¬å·å†…çš„JSONå†…å®¹
                    json_match = re.search(r'\{[^{}]*"category"[^{}]*\}', result_text)
                    if json_match:
                        json_text = json_match.group(0)
                        tqdm.write(f"[æå–] æå–åˆ°JSONç‰‡æ®µ: {json_text}")
                        result_json = json.loads(json_text)
                        category = result_json.get("category", "å…¶ä»–")
                        tags = result_json.get("tags", [])
                        if category not in CATEGORIES:
                            category = "å…¶ä»–"
                        tqdm.write(f"[æˆåŠŸ] ç‰‡æ®µè§£ææˆåŠŸ: category={category}, tags={tags}")
                        return category, tags
                    
                    # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åˆ†ç±»å’Œæ ‡ç­¾
                    category_match = re.search(r'"category":\s*"([^"]+)"', result_text)
                    tags_match = re.search(r'"tags":\s*\[(.*?)\]', result_text)
                    
                    if category_match:
                        category = category_match.group(1)
                        if category not in CATEGORIES:
                            category = "å…¶ä»–"
                        
                        tags = []
                        if tags_match:
                            tags_str = tags_match.group(1)
                            # æå–æ ‡ç­¾
                            tag_matches = re.findall(r'"([^"]+)"', tags_str)
                            tags = tag_matches[:5]  # æœ€å¤š5ä¸ªæ ‡ç­¾
                        
                        tqdm.write(f"[æˆåŠŸ] æ­£åˆ™æå–æˆåŠŸ: category={category}, tags={tags}")
                        return category, tags
                    
                    # å¦‚æœæ‰€æœ‰JSONæå–éƒ½å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦ç›´æ¥è¿”å›äº†åˆ†ç±»åç§°
                    for cat in CATEGORIES:
                        if cat in result_text:
                            tqdm.write(f"[æˆåŠŸ] æ‰¾åˆ°åˆ†ç±»åç§°: {cat}")
                            return cat, []
                    
                    tqdm.write(f"[é”™è¯¯] æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›'å…¶ä»–'")
                    return "å…¶ä»–", []
                    
                except Exception as e:
                    tqdm.write(f"[é”™è¯¯] JSONæå–è¿‡ç¨‹å‡ºé”™: {type(e).__name__}: {str(e)}")
                    return "å…¶ä»–", []
        else:
            # åªè¿›è¡Œåˆ†ç±»
            category = result_text if result_text in CATEGORIES else "å…¶ä»–"
            tqdm.write(f"[æˆåŠŸ] ä»…åˆ†ç±»æ¨¡å¼: {category}")
            return category, []
            
    except Exception as e:
        tqdm.write(f"[é”™è¯¯] classify_with_content_and_tags å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {str(e)}")
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©é‡è¯•æœºåˆ¶å¤„ç†
        raise


def classify_title_with_content(title: str, content: str = "") -> Tuple[str, List[str]]:
    """å°è£…å¸¦é‡è¯•çš„åˆ†ç±»å‡½æ•°ï¼Œå¤„ç†æœ€ç»ˆçš„å¼‚å¸¸"""
    tqdm.write(f"[åˆ†ç±»] å¼€å§‹åˆ†ç±»: {title[:50]}...")
    
    try:
        result = classify_with_content_and_tags(title, content)
        tqdm.write(f"[æˆåŠŸ] åˆ†ç±»æˆåŠŸ: {title[:50]} -> {result[0]} (æ ‡ç­¾: {result[1]})")
        return result
    except Exception as e:
        error_type = type(e).__name__
        error_msg = str(e)
        
        # è¯¦ç»†çš„é”™è¯¯åˆ†æ
        if "RetryError" in error_type:
            tqdm.write(f"[é”™è¯¯] é‡è¯•å¤±è´¥ (RetryError): {title[:50]}")
            tqdm.write(f"   åŸå§‹é”™è¯¯: {error_msg}")
            # å°è¯•ä»RetryErrorä¸­æå–åŸå§‹å¼‚å¸¸
            if hasattr(e, 'last_attempt') and hasattr(e.last_attempt, 'exception'):
                original_error = e.last_attempt.exception()
                tqdm.write(f"   æ ¹æœ¬åŸå› : {type(original_error).__name__}: {str(original_error)}")
        elif isinstance(e, requests.exceptions.ProxyError):
            tqdm.write(f"[é”™è¯¯] ä»£ç†è¿æ¥é”™è¯¯: {title[:50]} - {error_msg}")
            tqdm.write("  [å»ºè®®] æ£€æŸ¥ä»£ç†è®¾ç½®æˆ–ç¦ç”¨ä»£ç†")
        elif isinstance(e, requests.exceptions.Timeout):
            tqdm.write(f"[é”™è¯¯] è¯·æ±‚è¶…æ—¶: {title[:50]} - {error_msg}")
            tqdm.write(f"  [å»ºè®®] å½“å‰è¶…æ—¶è®¾ç½®ä¸º{API_TIMEOUT}ç§’ï¼Œå¯è€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´")
        elif isinstance(e, requests.exceptions.ConnectionError):
            tqdm.write(f"[é”™è¯¯] è¿æ¥é”™è¯¯: {title[:50]} - {error_msg}")
            tqdm.write("  [å»ºè®®] æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIç«¯ç‚¹")
        elif isinstance(e, requests.exceptions.HTTPError):
            if hasattr(e, 'response') and e.response is not None:
                status_code = e.response.status_code
                if status_code == 400:
                    tqdm.write(f"[é”™è¯¯] 400 Bad Request (ä¸é‡è¯•): {title[:50]} - {error_msg}")
                elif status_code == 401:
                    tqdm.write(f"[é”™è¯¯] 401 Unauthorized: {title[:50]} - APIå¯†é’¥å¯èƒ½æ— æ•ˆ")
                    tqdm.write("  [å»ºè®®] æ£€æŸ¥OPENAI_API_KEYé…ç½®")
                elif status_code == 429:
                    tqdm.write(f"[é”™è¯¯] 429 Rate Limit: {title[:50]} - APIè°ƒç”¨é¢‘ç‡è¿‡é«˜")
                    tqdm.write("  [å»ºè®®] é™ä½å¹¶å‘æ•°æˆ–å¢åŠ é‡è¯•é—´éš”")
                else:
                    tqdm.write(f"[é”™è¯¯] HTTP {status_code}: {title[:50]} - {error_msg}")
            else:
                tqdm.write(f"[é”™è¯¯] HTTPError (æ— å“åº”å¯¹è±¡): {title[:50]} - {error_msg}")
        else:
            tqdm.write(f"[é”™è¯¯] æœªçŸ¥é”™è¯¯ ({error_type}): {title[:50]} - {error_msg}")
            # æ‰“å°å¼‚å¸¸çš„è¯¦ç»†ä¿¡æ¯
            import traceback
            tqdm.write(f"   å¼‚å¸¸è·Ÿè¸ª: {traceback.format_exc()}")
        
        return "å…¶ä»–", []


def process_file(filename: str, result_manager: ResultManager) -> Tuple[str, bool]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›å¤„ç†ç»“æœå­—ç¬¦ä¸²å’Œæ˜¯å¦åº”è¯¥åœæ­¢çš„æ ‡å¿—"""
    start_time = time.time()
    src_path = os.path.join(SOURCE_DIR, filename)
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å¼€å§‹åˆ†éš”çº¿
    tqdm.write("\n" + "="*80)
    tqdm.write(f"[æ–‡ä»¶] å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
    tqdm.write("="*80)
    
    # æ£€æŸ¥åœæ­¢ä¿¡å·
    if stop_processing.is_set():
        tqdm.write("[åœæ­¢] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
        tqdm.write("="*80 + "\n")
        return f"åœæ­¢: {filename} - ç”¨æˆ·è¯·æ±‚åœæ­¢", True
    
    # æ£€æŸ¥æš‚åœä¿¡å·
    while pause_processing.is_set() and not stop_processing.is_set():
        time.sleep(0.5)  # ç­‰å¾…æ¢å¤
    
    # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆå¯èƒ½åœ¨æš‚åœæœŸé—´è®¾ç½®äº†åœæ­¢ï¼‰
    if stop_processing.is_set():
        tqdm.write("[åœæ­¢] æš‚åœæœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
        tqdm.write("="*80 + "\n")
        return f"åœæ­¢: {filename} - ç”¨æˆ·è¯·æ±‚åœæ­¢", True
    
    if not filename.endswith(".md"):
        result = ProcessResult(
            filename=filename,
            original_title=filename,
            category="è·³è¿‡",
            tags=[],
            process_time=time.time() - start_time,
            success=False,
            error_message="éMarkdownæ–‡ä»¶",
            processing_status="è·³è¿‡"
        )
        result_manager.add_result(result)
        tqdm.write("[è·³è¿‡] è·³è¿‡éMarkdownæ–‡ä»¶")
        tqdm.write("="*80 + "\n")
        return f"è·³è¿‡éMarkdownæ–‡ä»¶: {filename}", False

    title = os.path.splitext(filename)[0]
    category = "å…¶ä»–"
    tags = []
    content = ""
    frontmatter_data = {}
    
    # åˆå§‹åŒ–æ‰€æœ‰å­—æ®µ
    fm_title = None
    fm_url = None
    fm_author = None
    fm_author_badge = None
    fm_created = None
    fm_modified = None
    fm_upvote_num = None
    fm_comment_num = None
    word_count = None
    content_summary = ""

    try:
        # åœ¨å…³é”®æ­¥éª¤å‰æ£€æŸ¥åœæ­¢ä¿¡å·
        if stop_processing.is_set():
            return f"åœæ­¢: {filename} - ç”¨æˆ·è¯·æ±‚åœæ­¢", True
        # è¯»å–æ–‡ä»¶å†…å®¹å’Œfrontmatter
        if ENABLE_CONTENT_ANALYSIS:
            content, frontmatter_data = read_markdown_with_frontmatter(src_path)
        else:
            # å³ä½¿ä¸å¯ç”¨å†…å®¹åˆ†æï¼Œä¹Ÿè§£æfrontmatterè·å–å…ƒæ•°æ®
            _, frontmatter_data = read_markdown_with_frontmatter(src_path)
        
        # æå–frontmatterå­—æ®µ
        if frontmatter_data:
            fm_title = frontmatter_data.get('title', '')
            fm_url = frontmatter_data.get('url', '')
            fm_author = frontmatter_data.get('author', '')
            fm_author_badge = frontmatter_data.get('author_badge', '')
            fm_created = frontmatter_data.get('created', '')
            fm_modified = frontmatter_data.get('modified', '')
            
            # å¤„ç†æ•°å­—å­—æ®µï¼Œç¡®ä¿å®‰å…¨è½¬æ¢
            try:
                upvote_raw = frontmatter_data.get('upvote_num')
                fm_upvote_num = int(upvote_raw) if upvote_raw is not None else None
            except (ValueError, TypeError):
                fm_upvote_num = None
                
            try:
                comment_raw = frontmatter_data.get('comment_num')
                fm_comment_num = int(comment_raw) if comment_raw is not None else None
            except (ValueError, TypeError):
                fm_comment_num = None
        
        # è®¡ç®—å­—æ•°ç»Ÿè®¡å’Œå†…å®¹æ‘˜è¦
        if content:
            word_count = len(content.replace(' ', '').replace('\n', ''))
            content_summary = content[:200] + "..." if len(content) > 200 else content
        
        # ä½¿ç”¨frontmatterä¸­çš„titleï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–è€…æ–‡ä»¶åä½œä¸ºåˆ†ç±»ä¾æ®
        classification_title = fm_title if fm_title else title
        
        # åœ¨åˆ†ç±»å‰æ£€æŸ¥åœæ­¢ä¿¡å·
        if stop_processing.is_set():
            return f"åœæ­¢: {filename} - ç”¨æˆ·è¯·æ±‚åœæ­¢", True
        
        tqdm.write(f"[æ ‡é¢˜] åˆ†ç±»æ ‡é¢˜: {classification_title}")
        if content:
            tqdm.write(f"[å†…å®¹] å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        else:
            tqdm.write("[å†…å®¹] æ— å†…å®¹åˆ†æ")
        
        # è¿›è¡Œåˆ†ç±»å’Œæ ‡ç­¾æå–
        try:
            category, raw_tags = classify_title_with_content(classification_title, content)
            tqdm.write(f"[ç»“æœ] AIåˆ†ç±»ç»“æœ: category='{category}', raw_tags={raw_tags}")
        except Exception as e:
            tqdm.write(f"[é”™è¯¯] åˆ†ç±»è¿‡ç¨‹å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸è®©å¤–å±‚å¤„ç†
            raise
        
        # éªŒè¯åˆ†ç±»ç»“æœ
        if not category:
            tqdm.write(f"[è­¦å‘Š] ç©ºåˆ†ç±»ç»“æœï¼Œä½¿ç”¨'å…¶ä»–'")
            category = "å…¶ä»–"
        elif category not in CATEGORIES + ["å…¶ä»–"]:
            tqdm.write(f"[è­¦å‘Š] æ— æ•ˆåˆ†ç±» '{category}'ï¼Œä½¿ç”¨'å…¶ä»–'")
            category = "å…¶ä»–"
        
        # åœ¨ç§»åŠ¨æ–‡ä»¶å‰æ£€æŸ¥åœæ­¢ä¿¡å·
        if stop_processing.is_set():
            return f"åœæ­¢: {filename} - ç”¨æˆ·è¯·æ±‚åœæ­¢", True
        
        # æ ‡ç­¾è§„èŒƒåŒ–
        if ENABLE_TAG_EXTRACTION and raw_tags:
            tags = normalize_tags(raw_tags, category)
            tqdm.write(f"[æ ‡ç­¾] æ ‡ç­¾è§„èŒƒåŒ–: {raw_tags} -> {tags}")
        else:
            tags = []

        tqdm.write(f"[ç§»åŠ¨] å‡†å¤‡ç§»åŠ¨æ–‡ä»¶åˆ°åˆ†ç±»: {category}")
        
        # ç§»åŠ¨æ–‡ä»¶
        try:
            dest_dir = os.path.join(BASE_DIR, category)
            os.makedirs(dest_dir, exist_ok=True)
            dest_file_path = os.path.join(dest_dir, filename)
            
            tqdm.write(f"[è·¯å¾„] ç›®æ ‡è·¯å¾„: {dest_file_path}")
            shutil.move(src_path, dest_file_path)
            tqdm.write(f"[æˆåŠŸ] æ–‡ä»¶ç§»åŠ¨æˆåŠŸ: {filename} -> {category}")
            
        except Exception as e:
            tqdm.write(f"[é”™è¯¯] æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {type(e).__name__}: {str(e)}")
            raise
        
        # è®°å½•æˆåŠŸç»“æœ
        result = ProcessResult(
            filename=filename,
            original_title=title,
            category=category,
            tags=tags,
            process_time=time.time() - start_time,
            success=True,
            content_preview=content_summary,
            # Frontmatterå­—æ®µ
            title=fm_title,
            url=fm_url,
            author=fm_author,
            author_badge=fm_author_badge,
            created=fm_created,
            modified=fm_modified,
            upvote_num=fm_upvote_num,
            comment_num=fm_comment_num,
            # ç»Ÿè®¡å­—æ®µ
            word_count=word_count,
            content_summary=content_summary,
            processing_status="æˆåŠŸ"
        )
        result_manager.add_result(result)
        
        display_title = fm_title if fm_title else title
        tqdm.write(f"[æˆåŠŸ] å¤„ç†æˆåŠŸ: {display_title} -> {category}")
        if tags:
            tqdm.write(f"[æ ‡ç­¾] æ ‡ç­¾: {', '.join(tags)}")
        tqdm.write(f"[æ—¶é—´] å¤„ç†æ—¶é—´: {time.time() - start_time:.3f} ç§’")
        tqdm.write("="*80 + "\n")
        return f"æˆåŠŸ: {display_title} -> {category} (æ ‡ç­¾: {', '.join(tags) if tags else 'æ— '})", False
        
    except Exception as e:
        # è®°å½•å¤±è´¥ç»“æœ
        result = ProcessResult(
            filename=filename,
            original_title=title,
            category=category,
            tags=tags,
            process_time=time.time() - start_time,
            success=False,
            error_message=str(e),
            content_preview=content_summary,
            # Frontmatterå­—æ®µï¼ˆå³ä½¿å¤±è´¥ä¹Ÿå°è¯•ä¿å­˜å·²è§£æçš„æ•°æ®ï¼‰
            title=fm_title,
            url=fm_url,
            author=fm_author,
            author_badge=fm_author_badge,
            created=fm_created,
            modified=fm_modified,
            upvote_num=fm_upvote_num,
            comment_num=fm_comment_num,
            # ç»Ÿè®¡å­—æ®µ
            word_count=word_count,
            content_summary=content_summary,
            processing_status="å¤±è´¥"
        )
        result_manager.add_result(result)
        
        tqdm.write(f"[é”™è¯¯] æ–‡ä»¶å¤„ç†å¤±è´¥: {filename}")
        tqdm.write(f"[åˆ†ç±»] ç›®æ ‡åˆ†ç±»: {category}")
        tqdm.write(f"[è­¦å‘Š] é”™è¯¯ä¿¡æ¯: {str(e)}")
        tqdm.write(f"[æ—¶é—´] å¤„ç†æ—¶é—´: {time.time() - start_time:.3f} ç§’")
        tqdm.write("="*80 + "\n")
        return f"å¤±è´¥: {filename} - {str(e)} (åˆ†ç±»å°è¯•: {category})", False


def main():
    print("=== çŸ¥ä¹æ–‡ç« æ™ºèƒ½åˆ†ç±»å™¨ å¢å¼ºç‰ˆ ===")
    print(f"è¿è¡Œæ¨¡å¼: {'é«˜çº§åŠŸèƒ½æ¨¡å¼' if ENABLE_ADVANCED_FEATURES else 'è½»é‡çº§æ¨¡å¼'}")
    print(f"Pandasæ”¯æŒ: {'å¯ç”¨' if PANDAS_AVAILABLE else 'ä¸å¯ç”¨'}")
    print(f"å†…å®¹åˆ†æ: {'å¯ç”¨' if ENABLE_CONTENT_ANALYSIS else 'ç¦ç”¨'}")
    print(f"æ ‡ç­¾æå–: {'å¯ç”¨' if ENABLE_TAG_EXTRACTION else 'ç¦ç”¨'}")
    print(f"ç»“æœå¯¼å‡º: {'å¯ç”¨' if ENABLE_RESULT_EXPORT else 'ç¦ç”¨'}")
    print(f"ä¼˜é›…åœæ­¢: {'å¯ç”¨' if ENABLE_GRACEFUL_STOP else 'ç¦ç”¨'}")
    print(f"æš‚åœ/ç»§ç»­: {'å¯ç”¨' if ENABLE_PAUSE_RESUME else 'ç¦ç”¨'}")
    if ENABLE_RESULT_EXPORT:
        export_modes = []
        if PANDAS_AVAILABLE and "csv" in EXPORT_FORMAT:
            export_modes.append("CSV")
        if PANDAS_AVAILABLE and "excel" in EXPORT_FORMAT:
            export_modes.append("Excel")
        if not PANDAS_AVAILABLE:
            export_modes.append("æ–‡æœ¬æŠ¥å‘Š")
        print(f"å¯¼å‡ºæ ¼å¼: {', '.join(export_modes) if export_modes else 'æ— '}")
    print(f"å†…å®¹é•¿åº¦é™åˆ¶: {MAX_CONTENT_LENGTH} å­—ç¬¦")
    print(f"Frontmatterè§£æ: å¯ç”¨")
    print()
    
    if not PANDAS_AVAILABLE and ENABLE_ADVANCED_FEATURES:
        print("[è­¦å‘Š] æ³¨æ„: å› ä¸ºpandaså¯¼å…¥å¤±è´¥ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡çº§æ¨¡å¼")
        print("   - å°†ä¸æä¾›CSV/Excelå¯¼å‡ºåŠŸèƒ½")
        print("   - ä½¿ç”¨æ–‡æœ¬æŠ¥å‘Šä»£æ›¿è¯¦ç»†ç»Ÿè®¡")
        print("   - å»ºè®®æ£€æŸ¥å¹¶é‡æ–°å®‰è£…ä¾èµ–: pip install -r requirements.txt")
        print()
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    stop_handler.setup_signal_handlers()
    
    # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
    progress_manager = ProgressManager(BASE_DIR)
    
    # åˆå§‹åŒ–ç”¨æˆ·è¾“å…¥å¤„ç†å™¨
    input_handler = UserInputHandler()
    
    # åˆå§‹åŒ–ç»“æœç®¡ç†å™¨
    result_manager = ResultManager()
    
    # åˆå§‹åŒ–ç›®å½•
    os.makedirs(BASE_DIR, exist_ok=True)
    all_target_categories = CATEGORIES + ["å…¶ä»–"]
    for category_name in all_target_categories:
        os.makedirs(os.path.join(BASE_DIR, category_name), exist_ok=True)

    # è·å–æ–‡ä»¶åˆ—è¡¨
    try:
        files = [f for f in os.listdir(SOURCE_DIR) if os.path.isfile(os.path.join(SOURCE_DIR, f)) and f.endswith(".md")]
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæºç›®å½• {SOURCE_DIR} æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ SOURCE_DIR é…ç½®ã€‚")
        input("æŒ‰ä»»æ„é”®é€€å‡º...")
        return

    if not files:
        print(f"åœ¨ {SOURCE_DIR} ä¸­æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶ã€‚")
        input("æŒ‰ä»»æ„é”®é€€å‡º...")
        return

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„è¿›åº¦
    processed_files, failed_files = progress_manager.load_progress()
    
    # è¿‡æ»¤æ‰å·²å¤„ç†çš„æ–‡ä»¶
    remaining_files = [f for f in files if f not in processed_files and f not in failed_files]
    
    if len(remaining_files) < len(files):
        print(f"[è¿›åº¦] è¿›åº¦æ¢å¤: æ€»å…± {len(files)} ä¸ªæ–‡ä»¶ï¼Œå·²å¤„ç† {len(processed_files)} ä¸ªï¼Œå¤±è´¥ {len(failed_files)} ä¸ª")
        print(f"è¿˜éœ€å¤„ç† {len(remaining_files)} ä¸ªæ–‡ä»¶")
        
        if remaining_files:
            choice = input("æ˜¯å¦ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Ÿ(Y/n): ").strip().lower()
            if choice in ['n', 'no', 'å¦']:
                print("å–æ¶ˆå¤„ç†")
                return
        else:
            print("[å®Œæˆ] æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆï¼")
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if result_manager.results:
                stats = result_manager.get_statistics()
                print(f"æˆåŠŸ: {stats['success_count']}, å¤±è´¥: {stats['failure_count']}")
            input("æŒ‰ä»»æ„é”®é€€å‡º...")
            return
    
    print(f"å‘ç° {len(remaining_files)} ä¸ª .md å¾…å¤„ç†æ–‡ä»¶ï¼Œå°†ä» {SOURCE_DIR} ç§»åŠ¨åˆ° {BASE_DIR} ä¸‹çš„åˆ†ç±»ç›®å½•ã€‚")

    if not remaining_files:
        print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        input("æŒ‰ä»»æ„é”®é€€å‡º...")
        return

    # å¯åŠ¨ç”¨æˆ·è¾“å…¥ç›‘å¬
    input_handler.start_input_monitoring()
    
    # ä½¿ç”¨å•çº¿ç¨‹å¤„ç†ä»¥æ”¯æŒæ›´å¥½çš„åœæ­¢æ§åˆ¶
    print("å¼€å§‹å¤„ç†æ–‡ä»¶...")
    
    try:
        processed_count = 0
        current_processed = set(processed_files)
        current_failed = set(failed_files)
        
        with tqdm(total=len(remaining_files), desc="æ–‡ä»¶åˆ†ç±»ä¸­") as pbar:
            for filename in remaining_files:
                # æ£€æŸ¥åœæ­¢ä¿¡å·
                if stop_processing.is_set():
                    print(f"\n[è­¦å‘Š] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
                    break
                
                # å¤„ç†æ–‡ä»¶
                result_msg, should_stop = process_file(filename, result_manager)
                processed_count += 1
                
                # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                pbar.set_postfix_str(f"å½“å‰: {filename[:30]}...")
                pbar.update(1)
                
                # æ ¹æ®ç»“æœæ›´æ–°é›†åˆ
                if "æˆåŠŸ:" in result_msg:
                    current_processed.add(filename)
                elif "å¤±è´¥:" in result_msg:
                    current_failed.add(filename)
                
                # å®šæœŸä¿å­˜è¿›åº¦
                if processed_count % SAVE_PROGRESS_INTERVAL == 0:
                    progress_manager.save_progress(current_processed, current_failed)
                    tqdm.write(f"[ä¿å­˜] è¿›åº¦å·²ä¿å­˜ ({processed_count}/{len(remaining_files)})")
                
                # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·å°±é€€å‡º
                if should_stop:
                    print(f"\n[è­¦å‘Š] å¤„ç†ä¸­æ–­ï¼Œå·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
                    break
                
                # çŸ­æš‚ä¼‘æ¯ï¼Œå…è®¸å“åº”åœæ­¢ä¿¡å·
                time.sleep(0.01)
        
        # æœ€ç»ˆä¿å­˜è¿›åº¦
        progress_manager.save_progress(current_processed, current_failed)
        
    except KeyboardInterrupt:
        print(f"\n[è­¦å‘Š] ç”¨æˆ·ä¸­æ–­ï¼Œå·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
    except Exception as e:
        print(f"\n[é”™è¯¯] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # åœæ­¢ç”¨æˆ·è¾“å…¥ç›‘å¬
        input_handler.stop_input_monitoring()
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        if 'current_processed' in locals() and 'current_failed' in locals():
            progress_manager.save_progress(current_processed, current_failed)

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = result_manager.get_statistics()
    
    # æ‰“å°æ€»ç»“ä¿¡æ¯
    print("\n" + "="*50)
    print("å¤„ç†ç»“æœæ€»ç»“")
    print("="*50)
    print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"æˆåŠŸå¤„ç†: {stats['success_count']}")
    print(f"å¤±è´¥å¤„ç†: {stats['failure_count']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"æ€»å¤„ç†æ—¶é—´: {stats['processing_time']['total_time']:.2f} ç§’")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['processing_time']['average_per_file']:.3f} ç§’/æ–‡ä»¶")
    
    # åˆ†ç±»åˆ†å¸ƒ
    if stats['category_distribution']:
        print("\nåˆ†ç±»åˆ†å¸ƒ:")
        for category, count in sorted(stats['category_distribution'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {category}: {count} ç¯‡")
    
    # å¯¼å‡ºç»“æœ
    if ENABLE_RESULT_EXPORT:
        print(f"\næ­£åœ¨å¯¼å‡ºç»“æœ...")
        result_manager.export_results(BASE_DIR)
    
    # å¤„ç†å®Œæˆæˆ–ä¸­æ–­åçš„æ¸…ç†
    if stop_processing.is_set():
        print(f"\n[åœæ­¢] å¤„ç†å·²åœæ­¢")
        print("è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶å¯ä»¥ä»ä¸­æ–­å¤„ç»§ç»­")
    else:
        print(f"\n[å®Œæˆ] åˆ†ç±»å®Œæˆï¼")
        # å®Œæˆåæ¸…é™¤è¿›åº¦æ–‡ä»¶
        try:
            choice = input("æ˜¯å¦æ¸…é™¤è¿›åº¦æ–‡ä»¶ï¼Ÿ(Y/n): ").strip().lower()
            if choice not in ['n', 'no', 'å¦']:
                progress_manager.clear_progress()
        except:
            pass
    
    if stats['failure_count'] > 0:
        print("è¯·æ£€æŸ¥ä¸Šé¢ç”± tqdm.write è¾“å‡ºçš„å¤±è´¥è¯¦æƒ…ã€‚")
    
    input("æŒ‰ä»»æ„é”®é€€å‡º...")


if __name__ == "__main__":
    main()
